{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _Speech Processing Labs: TTS_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Install some more python packages</h3>\n",
    "This notebook needs a few more python packages further to those used in the SIGNALs notebooks. You can install them using <strong>conda</strong> in your terminal. First make sure your conda environment is active:\n",
    "\n",
    "<pre>\n",
    "<code>\n",
    "conda activate slp\n",
    "</code>\n",
    "</pre>\n",
    "    \n",
    "\n",
    "Or replace 'slp' with the name of the conda enviroment you use with these notebooks. \n",
    "Then run this command to install the packages: \n",
    "\n",
    "<pre>\n",
    "<code>\n",
    "conda install -c conda-forge anytree urllib3 requests\n",
    "</code>\n",
    "</pre>\n",
    "\n",
    "<strong>Alternatively</strong>, you could also install them using <strong>pip</strong>\n",
    "\n",
    "<pre>\n",
    "<code>\n",
    "pip install anytree urllib3 requests\n",
    "</code>\n",
    "</pre>\n",
    "You may then need to restart the kernel for this notebook: go to 'Kernel' in the menu and then 'restart and clear output'. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting anytree\n",
      "  Using cached anytree-2.8.0-py2.py3-none-any.whl (41 kB)\n",
      "Collecting urllib3\n",
      "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
      "\u001b[K     |████████████████████████████████| 138 kB 4.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests\n",
      "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[K     |████████████████████████████████| 62 kB 1.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /Users/clai/miniconda3/envs/slp/lib/python3.8/site-packages (from anytree) (1.16.0)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /Users/clai/miniconda3/envs/slp/lib/python3.8/site-packages (from requests) (2021.5.30)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Downloading charset_normalizer-2.0.7-py3-none-any.whl (38 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, requests, anytree\n",
      "Successfully installed anytree-2.8.0 charset-normalizer-2.0.7 idna-3.3 requests-2.26.0 urllib3-1.26.7\n"
     ]
    }
   ],
   "source": [
    "!pip install anytree urllib3 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this first after you've installed all the imported packages (see above) \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import string\n",
    "import IPython\n",
    "import urllib.request\n",
    "from anytree import Node, RenderTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(counts):\n",
    "    H=0 # entropy\n",
    "    total_count=float(sum(counts))\n",
    "    for c in counts:\n",
    "        if c > 0: # cannot take log of zero\n",
    "            p=float(c)/total_count\n",
    "            H=H + p * math.log2(p)\n",
    "    H=H*-1.0\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Learning a complete decision tree\n",
    "\n",
    "### Learning Outcomes\n",
    "* Relate what you have learned in the previous notebooks to the full algorithm\n",
    "* Understand a variety of stopping criteria\n",
    "\n",
    "### Need to know\n",
    "* The algorithm for learning a decision tree\n",
    "* Topic Videos: Pronunciation, Decision tree, Learning decision trees\n",
    "\n",
    "\n",
    "In the previous notebook, you ran a few iterations of the tree learning (\"growing\") algorithm manually. It was hard work, wasn't it! This notebook provides a complete implementation of the algorithm that runs automatically. This is a simple algorithm that matches the one described in the topic video [Learning Decision Trees](http://speech.zone/courses/speech-processing/module-4-speech-synthesis-front-end-2/videos/learning-decision-trees/). There are many possible improvements to this basic algorithm, such as more sophisticated stopping criteria, but those are beyond the scope of the course.\n",
    "\n",
    "Try to understand the code, remembering that this is not a requirement of the course. Even if you don't understand the code, try experimenting with the stopping criteria to discover the effect on the resulting tree. Classify the test data with each tree that you build.\n",
    "\n",
    "We're going back to the G2P task for this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Data\n",
    "\n",
    "### 3.1.1 Raw data\n",
    "\n",
    "We will use CMUdict as a source of raw data, so let's download a copy - run this code block and wait for it to print the message \"loaded CMUdict\". This will take a moment or two, depending on your download speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded CMUdict\n"
     ]
    }
   ],
   "source": [
    "cmudict_url=\"http://svn.code.sf.net/p/cmusphinx/code/trunk/cmudict/cmudict.0.7a\" # 4 MB download\n",
    "with urllib.request.urlopen(cmudict_url) as f:\n",
    "    cmudict=f.readlines()\n",
    "print(\"loaded CMUdict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Feature extraction and feature engineering\n",
    "\n",
    "Now we extract the predictors and predictee. To keep this example simple, we are only using CMUdict entries that contain a single occurrence of the letter 'c' and contain a single occurrence of one of the 4 phonemes /k/, /s/, /ʃ/, or /tʃ/. We are assuming that the letter and the phoneme correspond. As usual, we will get around the problem of words having a variable number of letters by placing a window around the letter of interest.\n",
    "\n",
    "In reality, we would want to use all words containing one or more letter 'c', and this would involve making an alignment between letters and phonemes (e.g., using dynamic programming) to find this correspondence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared the data. Here's what it looks like:\n",
      "[('aache', 'k'), ('aache', 'k'), ('ancor', 'k'), ('bacha', 'k'), ('back_', 'k')] ... [('zycad', 'k'), ('zych_', 'tʃ'), ('zyche', 'k'), ('deco_', 'k'), ('wicki', 'k')]\n"
     ]
    }
   ],
   "source": [
    "discard=\";#(){}!?'\\\"%&,-.:/\"\n",
    "letter_of_interest=\"c\"\n",
    "phonemes_of_interest=[\"K\",\"SH\",\"S\",\"CH\"]\n",
    "\n",
    "data=[]\n",
    "\n",
    "for line in cmudict:\n",
    "    l=line.decode() # because urllib gives us a bytestream, not text\n",
    "    # discard messy entries and comments\n",
    "    if all(x not in l for x in discard):\n",
    "        # pad with two underscores for P PP and N NN when C is word initial or final\n",
    "        g=\"__\"+l.split(\"  \",1)[0].strip().lower()+\"__\"\n",
    "        # drop lexical stress and split phonemes into a list\n",
    "        p=l.split(\"  \",1)[1].replace(\"0\",\"\").replace(\"1\",\"\").replace(\"2\",\"\").strip().split()\n",
    "        # only keep words with a single occurence of letter_of_interest\n",
    "        if g.count(letter_of_interest) != 1: continue\n",
    "        # only keep words with a single occurence of any phoneme of interest\n",
    "        if sum(p.count(phoneme) for phoneme in phonemes_of_interest) !=1: continue\n",
    "        which_phoneme = [phoneme for phoneme in phonemes_of_interest if(phoneme in p)][0]\n",
    "        # map to the IPA because it looks prettier\n",
    "        which_phoneme = which_phoneme.replace(\"K\",\"k\").replace(\"SH\",\"ʃ\").replace(\"S\",\"s\").replace(\"CH\",\"tʃ\")\n",
    "        # place a window around the letter\n",
    "        where=g.find(letter_of_interest)\n",
    "        letters=g[where-2:where+3]\n",
    "        # extract a tuple of predictors and predictee; we'll store the predictors in a string for simplicity\n",
    "        data.append((letters,which_phoneme))\n",
    "\n",
    "# we will use the IPA from now on\n",
    "phonemes_of_interest=[\"k\",\"s\",\"ʃ\",\"tʃ\"]\n",
    "\n",
    "\n",
    "print(\"Prepared the data. Here's what it looks like:\")\n",
    "print(data[:5],\"...\",data[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 Questions\n",
    "\n",
    "We construct a list of all possible questions that we could ask about the predictors. Each predictor value is one of the 26 letters of the alphabet, or \"_\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PP==\"a\"': re.compile(r'a....', re.UNICODE),\n",
       " 'P==\"a\"': re.compile(r'.a...', re.UNICODE),\n",
       " 'N==\"a\"': re.compile(r'...a.', re.UNICODE),\n",
       " 'NN==\"a\"': re.compile(r'....a', re.UNICODE),\n",
       " 'PP==\"b\"': re.compile(r'b....', re.UNICODE),\n",
       " 'P==\"b\"': re.compile(r'.b...', re.UNICODE),\n",
       " 'N==\"b\"': re.compile(r'...b.', re.UNICODE),\n",
       " 'NN==\"b\"': re.compile(r'....b', re.UNICODE),\n",
       " 'PP==\"c\"': re.compile(r'c....', re.UNICODE),\n",
       " 'P==\"c\"': re.compile(r'.c...', re.UNICODE),\n",
       " 'N==\"c\"': re.compile(r'...c.', re.UNICODE),\n",
       " 'NN==\"c\"': re.compile(r'....c', re.UNICODE),\n",
       " 'PP==\"d\"': re.compile(r'd....', re.UNICODE),\n",
       " 'P==\"d\"': re.compile(r'.d...', re.UNICODE),\n",
       " 'N==\"d\"': re.compile(r'...d.', re.UNICODE),\n",
       " 'NN==\"d\"': re.compile(r'....d', re.UNICODE),\n",
       " 'PP==\"e\"': re.compile(r'e....', re.UNICODE),\n",
       " 'P==\"e\"': re.compile(r'.e...', re.UNICODE),\n",
       " 'N==\"e\"': re.compile(r'...e.', re.UNICODE),\n",
       " 'NN==\"e\"': re.compile(r'....e', re.UNICODE),\n",
       " 'PP==\"f\"': re.compile(r'f....', re.UNICODE),\n",
       " 'P==\"f\"': re.compile(r'.f...', re.UNICODE),\n",
       " 'N==\"f\"': re.compile(r'...f.', re.UNICODE),\n",
       " 'NN==\"f\"': re.compile(r'....f', re.UNICODE),\n",
       " 'PP==\"g\"': re.compile(r'g....', re.UNICODE),\n",
       " 'P==\"g\"': re.compile(r'.g...', re.UNICODE),\n",
       " 'N==\"g\"': re.compile(r'...g.', re.UNICODE),\n",
       " 'NN==\"g\"': re.compile(r'....g', re.UNICODE),\n",
       " 'PP==\"h\"': re.compile(r'h....', re.UNICODE),\n",
       " 'P==\"h\"': re.compile(r'.h...', re.UNICODE),\n",
       " 'N==\"h\"': re.compile(r'...h.', re.UNICODE),\n",
       " 'NN==\"h\"': re.compile(r'....h', re.UNICODE),\n",
       " 'PP==\"i\"': re.compile(r'i....', re.UNICODE),\n",
       " 'P==\"i\"': re.compile(r'.i...', re.UNICODE),\n",
       " 'N==\"i\"': re.compile(r'...i.', re.UNICODE),\n",
       " 'NN==\"i\"': re.compile(r'....i', re.UNICODE),\n",
       " 'PP==\"j\"': re.compile(r'j....', re.UNICODE),\n",
       " 'P==\"j\"': re.compile(r'.j...', re.UNICODE),\n",
       " 'N==\"j\"': re.compile(r'...j.', re.UNICODE),\n",
       " 'NN==\"j\"': re.compile(r'....j', re.UNICODE),\n",
       " 'PP==\"k\"': re.compile(r'k....', re.UNICODE),\n",
       " 'P==\"k\"': re.compile(r'.k...', re.UNICODE),\n",
       " 'N==\"k\"': re.compile(r'...k.', re.UNICODE),\n",
       " 'NN==\"k\"': re.compile(r'....k', re.UNICODE),\n",
       " 'PP==\"l\"': re.compile(r'l....', re.UNICODE),\n",
       " 'P==\"l\"': re.compile(r'.l...', re.UNICODE),\n",
       " 'N==\"l\"': re.compile(r'...l.', re.UNICODE),\n",
       " 'NN==\"l\"': re.compile(r'....l', re.UNICODE),\n",
       " 'PP==\"m\"': re.compile(r'm....', re.UNICODE),\n",
       " 'P==\"m\"': re.compile(r'.m...', re.UNICODE),\n",
       " 'N==\"m\"': re.compile(r'...m.', re.UNICODE),\n",
       " 'NN==\"m\"': re.compile(r'....m', re.UNICODE),\n",
       " 'PP==\"n\"': re.compile(r'n....', re.UNICODE),\n",
       " 'P==\"n\"': re.compile(r'.n...', re.UNICODE),\n",
       " 'N==\"n\"': re.compile(r'...n.', re.UNICODE),\n",
       " 'NN==\"n\"': re.compile(r'....n', re.UNICODE),\n",
       " 'PP==\"o\"': re.compile(r'o....', re.UNICODE),\n",
       " 'P==\"o\"': re.compile(r'.o...', re.UNICODE),\n",
       " 'N==\"o\"': re.compile(r'...o.', re.UNICODE),\n",
       " 'NN==\"o\"': re.compile(r'....o', re.UNICODE),\n",
       " 'PP==\"p\"': re.compile(r'p....', re.UNICODE),\n",
       " 'P==\"p\"': re.compile(r'.p...', re.UNICODE),\n",
       " 'N==\"p\"': re.compile(r'...p.', re.UNICODE),\n",
       " 'NN==\"p\"': re.compile(r'....p', re.UNICODE),\n",
       " 'PP==\"q\"': re.compile(r'q....', re.UNICODE),\n",
       " 'P==\"q\"': re.compile(r'.q...', re.UNICODE),\n",
       " 'N==\"q\"': re.compile(r'...q.', re.UNICODE),\n",
       " 'NN==\"q\"': re.compile(r'....q', re.UNICODE),\n",
       " 'PP==\"r\"': re.compile(r'r....', re.UNICODE),\n",
       " 'P==\"r\"': re.compile(r'.r...', re.UNICODE),\n",
       " 'N==\"r\"': re.compile(r'...r.', re.UNICODE),\n",
       " 'NN==\"r\"': re.compile(r'....r', re.UNICODE),\n",
       " 'PP==\"s\"': re.compile(r's....', re.UNICODE),\n",
       " 'P==\"s\"': re.compile(r'.s...', re.UNICODE),\n",
       " 'N==\"s\"': re.compile(r'...s.', re.UNICODE),\n",
       " 'NN==\"s\"': re.compile(r'....s', re.UNICODE),\n",
       " 'PP==\"t\"': re.compile(r't....', re.UNICODE),\n",
       " 'P==\"t\"': re.compile(r'.t...', re.UNICODE),\n",
       " 'N==\"t\"': re.compile(r'...t.', re.UNICODE),\n",
       " 'NN==\"t\"': re.compile(r'....t', re.UNICODE),\n",
       " 'PP==\"u\"': re.compile(r'u....', re.UNICODE),\n",
       " 'P==\"u\"': re.compile(r'.u...', re.UNICODE),\n",
       " 'N==\"u\"': re.compile(r'...u.', re.UNICODE),\n",
       " 'NN==\"u\"': re.compile(r'....u', re.UNICODE),\n",
       " 'PP==\"v\"': re.compile(r'v....', re.UNICODE),\n",
       " 'P==\"v\"': re.compile(r'.v...', re.UNICODE),\n",
       " 'N==\"v\"': re.compile(r'...v.', re.UNICODE),\n",
       " 'NN==\"v\"': re.compile(r'....v', re.UNICODE),\n",
       " 'PP==\"w\"': re.compile(r'w....', re.UNICODE),\n",
       " 'P==\"w\"': re.compile(r'.w...', re.UNICODE),\n",
       " 'N==\"w\"': re.compile(r'...w.', re.UNICODE),\n",
       " 'NN==\"w\"': re.compile(r'....w', re.UNICODE),\n",
       " 'PP==\"x\"': re.compile(r'x....', re.UNICODE),\n",
       " 'P==\"x\"': re.compile(r'.x...', re.UNICODE),\n",
       " 'N==\"x\"': re.compile(r'...x.', re.UNICODE),\n",
       " 'NN==\"x\"': re.compile(r'....x', re.UNICODE),\n",
       " 'PP==\"y\"': re.compile(r'y....', re.UNICODE),\n",
       " 'P==\"y\"': re.compile(r'.y...', re.UNICODE),\n",
       " 'N==\"y\"': re.compile(r'...y.', re.UNICODE),\n",
       " 'NN==\"y\"': re.compile(r'....y', re.UNICODE),\n",
       " 'PP==\"z\"': re.compile(r'z....', re.UNICODE),\n",
       " 'P==\"z\"': re.compile(r'.z...', re.UNICODE),\n",
       " 'N==\"z\"': re.compile(r'...z.', re.UNICODE),\n",
       " 'NN==\"z\"': re.compile(r'....z', re.UNICODE),\n",
       " 'PP==\"_\"': re.compile(r'_....', re.UNICODE),\n",
       " 'P==\"_\"': re.compile(r'._...', re.UNICODE),\n",
       " 'N==\"_\"': re.compile(r'..._.', re.UNICODE),\n",
       " 'NN==\"_\"': re.compile(r'...._', re.UNICODE)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a dictionary of questions\n",
    "#  key is a pretty name for a question, e.g., P==\"h\"\n",
    "#  value is a regex that implements the question by matching the string of predictors, e.g., .h...\n",
    "questions={}\n",
    "# the predictors are stored in a single string, so we can write the questions as simple patterns (regular expressions)\n",
    "for letter in string.ascii_lowercase+\"_\":\n",
    "    questions[\"PP==\\\"\"+letter+\"\\\"\"]=re.compile(letter+\"....\")\n",
    "    questions[\"P==\\\"\"+letter+\"\\\"\"]=re.compile(\".\"+letter+\"...\")\n",
    "    # questions[\" C==\"+letter]=re.compile(\"..\"+letter+\"..\") not needed in this example since C is always the same letter\n",
    "    questions[\"N==\\\"\"+letter+\"\\\"\"]=re.compile(\"...\"+letter+\".\")\n",
    "    questions[\"NN==\\\"\"+letter+\"\\\"\"]=re.compile(\"....\"+letter)\n",
    "\n",
    "questions    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 The tree-growing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grow_tree(parent_node,data,questions,predictee_values):\n",
    "\n",
    "    best_question,entropy_reduction=find_best_question(data,questions,predictee_values)\n",
    "\n",
    "    if best_question != None:\n",
    "        yes_data,no_data = split_data(data,questions[best_question])\n",
    "    else:\n",
    "        yes_data=[]\n",
    "        no_data=[]\n",
    "        \n",
    "    # compute majority class, in case this is a terminal (leaf) node\n",
    "    #  (or to label non-terminal nodes out of interest)\n",
    "    max_count=0\n",
    "    majority_value=\"NONE!\"\n",
    "    total_count=0\n",
    "    distribution=\"\"\n",
    "    predictees = [data_point[1] for data_point in data]\n",
    "    for predictee_value in predictee_values:\n",
    "        this_count = predictees.count(predictee_value)\n",
    "        total_count = total_count + this_count\n",
    "        distribution=distribution+\" \"+predictee_value+\":\"+str(this_count)\n",
    "        if this_count > max_count:\n",
    "            max_count = this_count\n",
    "            majority_value = predictee_value\n",
    "\n",
    "    terminate_here=False\n",
    "    reason=\"\"\n",
    "    # Here are the stopping criteria - if any one of them is met, we stop\n",
    "    #  you can experiment with the values in some of these\n",
    "    if (best_question == None):\n",
    "        reason=reason+\" failed to find any question that would reduce entropy\\n\"\n",
    "        terminate_here=True\n",
    "    if (entropy_reduction < 0.2):\n",
    "        reason=reason+\" entropy gain would only be {:.3} bits\".format(entropy_reduction)+\"\\n\"\n",
    "        terminate_here=True\n",
    "    if (len(yes_data) < 200) or (len(no_data) < 200):\n",
    "        reason=reason+\" too few data would remain in one partition after best available split:\"+str(len(yes_data))+\"+\"+str(len(no_data))+\" data points.\\n\"\n",
    "        terminate_here=True\n",
    "    if terminate_here:\n",
    "        print(\"TERMINATING. One or more stopping criteria were met:\")\n",
    "        print(reason,end='')\n",
    "        print(\" creating leaf node labelled with class\",majority_value)\n",
    "        # make this node a leaf, labelled with the majority class\n",
    "        new_node = Node(\"/\"+majority_value+\"/ \"+distribution, parent=parent_node)\n",
    "\n",
    "        print(\"\\n======== Tree currently looks like this =========\")\n",
    "        for pre, fill, node in RenderTree(new_node.root):\n",
    "            print(\"%s%s\" % (pre, node.name))\n",
    "        print(\"=================================================\\n\")\n",
    "\n",
    "        return\n",
    "\n",
    "    else: \n",
    "        print(\"QUESTION\",best_question,\"will reduce entropy by {:.3} bits\".format(entropy_reduction),\"so adding this to the tree.\")\n",
    "        # label the current node with the best question\n",
    "        new_node = Node(best_question+\" \"+str(len(data))+\" /\"+majority_value+\"/ \"+distribution, parent=parent_node)\n",
    "\n",
    "        print(\"\\n======== Tree currently looks like this =========\")\n",
    "        for pre, fill, node in RenderTree(new_node.root):\n",
    "            print(\"%s%s\" % (pre, node.name))\n",
    "        print(\"=================================================\\n\")\n",
    "\n",
    "        # recurse\n",
    "        print(\"RECURSE from\",new_node.name.split(' ')[0],\"down the 'yes' branch\")\n",
    "        grow_tree(new_node,yes_data,questions,predictee_values)\n",
    "        print(\"RECURSE from\",new_node.name.split(' ')[0],\"down the 'no' branch\")\n",
    "        grow_tree(new_node, no_data,questions,predictee_values)\n",
    "\n",
    "    return new_node\n",
    "\n",
    "\n",
    "def split_data(data,question,only_predictees=False):\n",
    "\n",
    "    yes=[]\n",
    "    no=[]\n",
    "    for (predictors,predictee) in data:\n",
    "        if question.match(predictors):\n",
    "            if only_predictees:\n",
    "                yes.append(predictee)\n",
    "            else:\n",
    "                yes.append((predictors,predictee))\n",
    "        else:\n",
    "            if only_predictees:\n",
    "                no.append(predictee)\n",
    "            else:\n",
    "                no.append((predictors,predictee))\n",
    "    return yes,no\n",
    "\n",
    "def find_best_question(data,questions,predictee_values):\n",
    "\n",
    "    parent_predictees=[predictee for (predictors,predictee) in data]\n",
    "    parent_counts=[parent_predictees.count(phoneme) for phoneme in predictee_values]\n",
    "    parent_entropy=entropy(parent_counts)\n",
    "    best_entropy=parent_entropy\n",
    "    best_question=None\n",
    "    for this_question in questions:\n",
    "        yes_predictees,no_predictees = split_data(data,questions[this_question],only_predictees=True)\n",
    "        yes_counts=[yes_predictees.count(predictee) for predictee in predictee_values]\n",
    "        no_counts =[no_predictees.count(predictee) for predictee in predictee_values]\n",
    "        yes_count=sum(yes_counts)\n",
    "        no_count =sum(no_counts)\n",
    "        total_count = yes_count + no_count\n",
    "        yes_prob=float(yes_count)/float(total_count)\n",
    "        no_prob =float(no_count)/float(total_count)\n",
    "        this_entropy=yes_prob*entropy(yes_counts) + no_prob*entropy(no_counts)\n",
    "        if this_entropy < best_entropy:\n",
    "            best_entropy = this_entropy\n",
    "            best_question = this_question\n",
    "\n",
    "    return(best_question,parent_entropy-best_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Run the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to grow tree from 16540 data points using 108 questions\n",
      "QUESTION N==\"h\" will reduce entropy by 0.378 bits so adding this to the tree.\n",
      "\n",
      "======== Tree currently looks like this =========\n",
      "N==\"h\" 16540 /k/  k:11180 s:2185 ʃ:1170 tʃ:2005\n",
      "=================================================\n",
      "\n",
      "RECURSE from N==\"h\" down the 'yes' branch\n",
      "QUESTION P==\"s\" will reduce entropy by 0.484 bits so adding this to the tree.\n",
      "\n",
      "======== Tree currently looks like this =========\n",
      "N==\"h\" 16540 /k/  k:11180 s:2185 ʃ:1170 tʃ:2005\n",
      "└── P==\"s\" 3737 /tʃ/  k:949 s:9 ʃ:1028 tʃ:1751\n",
      "=================================================\n",
      "\n",
      "RECURSE from P==\"s\" down the 'yes' branch\n",
      "TERMINATING. One or more stopping criteria were met:\n",
      " too few data would remain in one partition after best available split:71+830 data points.\n",
      " creating leaf node labelled with class ʃ\n",
      "\n",
      "======== Tree currently looks like this =========\n",
      "N==\"h\" 16540 /k/  k:11180 s:2185 ʃ:1170 tʃ:2005\n",
      "└── P==\"s\" 3737 /tʃ/  k:949 s:9 ʃ:1028 tʃ:1751\n",
      "    └── /ʃ/  k:0 s:3 ʃ:827 tʃ:71\n",
      "=================================================\n",
      "\n",
      "RECURSE from P==\"s\" down the 'no' branch\n",
      "TERMINATING. One or more stopping criteria were met:\n",
      " entropy gain would only be 0.0748 bits\n",
      " creating leaf node labelled with class tʃ\n",
      "\n",
      "======== Tree currently looks like this =========\n",
      "N==\"h\" 16540 /k/  k:11180 s:2185 ʃ:1170 tʃ:2005\n",
      "└── P==\"s\" 3737 /tʃ/  k:949 s:9 ʃ:1028 tʃ:1751\n",
      "    ├── /ʃ/  k:0 s:3 ʃ:827 tʃ:71\n",
      "    └── /tʃ/  k:949 s:6 ʃ:201 tʃ:1680\n",
      "=================================================\n",
      "\n",
      "RECURSE from N==\"h\" down the 'no' branch\n",
      "QUESTION N==\"e\" will reduce entropy by 0.301 bits so adding this to the tree.\n",
      "\n",
      "======== Tree currently looks like this =========\n",
      "N==\"h\" 16540 /k/  k:11180 s:2185 ʃ:1170 tʃ:2005\n",
      "├── P==\"s\" 3737 /tʃ/  k:949 s:9 ʃ:1028 tʃ:1751\n",
      "│   ├── /ʃ/  k:0 s:3 ʃ:827 tʃ:71\n",
      "│   └── /tʃ/  k:949 s:6 ʃ:201 tʃ:1680\n",
      "└── N==\"e\" 12803 /k/  k:10231 s:2176 ʃ:142 tʃ:254\n",
      "=================================================\n",
      "\n",
      "RECURSE from N==\"e\" down the 'yes' branch\n",
      "TERMINATING. One or more stopping criteria were met:\n",
      " entropy gain would only be 0.163 bits\n",
      " too few data would remain in one partition after best available split:44+1457 data points.\n",
      " creating leaf node labelled with class s\n",
      "\n",
      "======== Tree currently looks like this =========\n",
      "N==\"h\" 16540 /k/  k:11180 s:2185 ʃ:1170 tʃ:2005\n",
      "├── P==\"s\" 3737 /tʃ/  k:949 s:9 ʃ:1028 tʃ:1751\n",
      "│   ├── /ʃ/  k:0 s:3 ʃ:827 tʃ:71\n",
      "│   └── /tʃ/  k:949 s:6 ʃ:201 tʃ:1680\n",
      "└── N==\"e\" 12803 /k/  k:10231 s:2176 ʃ:142 tʃ:254\n",
      "    └── /s/  k:46 s:1339 ʃ:12 tʃ:104\n",
      "=================================================\n",
      "\n",
      "RECURSE from N==\"e\" down the 'no' branch\n",
      "QUESTION N==\"i\" will reduce entropy by 0.301 bits so adding this to the tree.\n",
      "\n",
      "======== Tree currently looks like this =========\n",
      "N==\"h\" 16540 /k/  k:11180 s:2185 ʃ:1170 tʃ:2005\n",
      "├── P==\"s\" 3737 /tʃ/  k:949 s:9 ʃ:1028 tʃ:1751\n",
      "│   ├── /ʃ/  k:0 s:3 ʃ:827 tʃ:71\n",
      "│   └── /tʃ/  k:949 s:6 ʃ:201 tʃ:1680\n",
      "└── N==\"e\" 12803 /k/  k:10231 s:2176 ʃ:142 tʃ:254\n",
      "    ├── /s/  k:46 s:1339 ʃ:12 tʃ:104\n",
      "    └── N==\"i\" 11302 /k/  k:10185 s:837 ʃ:130 tʃ:150\n",
      "=================================================\n",
      "\n",
      "RECURSE from N==\"i\" down the 'yes' branch\n",
      "TERMINATING. One or more stopping criteria were met:\n",
      " too few data would remain in one partition after best available split:162+745 data points.\n",
      " creating leaf node labelled with class s\n",
      "\n",
      "======== Tree currently looks like this =========\n",
      "N==\"h\" 16540 /k/  k:11180 s:2185 ʃ:1170 tʃ:2005\n",
      "├── P==\"s\" 3737 /tʃ/  k:949 s:9 ʃ:1028 tʃ:1751\n",
      "│   ├── /ʃ/  k:0 s:3 ʃ:827 tʃ:71\n",
      "│   └── /tʃ/  k:949 s:6 ʃ:201 tʃ:1680\n",
      "└── N==\"e\" 12803 /k/  k:10231 s:2176 ʃ:142 tʃ:254\n",
      "    ├── /s/  k:46 s:1339 ʃ:12 tʃ:104\n",
      "    └── N==\"i\" 11302 /k/  k:10185 s:837 ʃ:130 tʃ:150\n",
      "        └── /s/  k:35 s:632 ʃ:126 tʃ:114\n",
      "=================================================\n",
      "\n",
      "RECURSE from N==\"i\" down the 'no' branch\n",
      "TERMINATING. One or more stopping criteria were met:\n",
      " entropy gain would only be 0.0869 bits\n",
      " too few data would remain in one partition after best available split:158+10237 data points.\n",
      " creating leaf node labelled with class k\n",
      "\n",
      "======== Tree currently looks like this =========\n",
      "N==\"h\" 16540 /k/  k:11180 s:2185 ʃ:1170 tʃ:2005\n",
      "├── P==\"s\" 3737 /tʃ/  k:949 s:9 ʃ:1028 tʃ:1751\n",
      "│   ├── /ʃ/  k:0 s:3 ʃ:827 tʃ:71\n",
      "│   └── /tʃ/  k:949 s:6 ʃ:201 tʃ:1680\n",
      "└── N==\"e\" 12803 /k/  k:10231 s:2176 ʃ:142 tʃ:254\n",
      "    ├── /s/  k:46 s:1339 ʃ:12 tʃ:104\n",
      "    └── N==\"i\" 11302 /k/  k:10185 s:837 ʃ:130 tʃ:150\n",
      "        ├── /s/  k:35 s:632 ʃ:126 tʃ:114\n",
      "        └── /k/  k:10150 s:205 ʃ:4 tʃ:36\n",
      "=================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting to grow tree from \"+str(len(data))+\" data points using \"+str(len(questions))+\" questions\")\n",
    "root_node=grow_tree(None,data,questions,phonemes_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the tree printed above, each non-terminal node is labelled with a question and the total number of data points that reached this node when learning the tree. See how the amount of data reduces as the tree is grown. This is a weakness of the Decision Tree as a model, because the lower parts of the tree are learned from much less data than the parts nearer the root. \n",
    "\n",
    "All nodes are labelled with the majority class and the distribution of predictee values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Experiment with the stopping criteria\n",
    "\n",
    "Try changing some of the parameters in the stopping criteria. You can make the tree smaller or larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Test\n",
    "\n",
    "Pass the following test data through the tree and predict the phoneme. This is called \"doing inference\" or simply \"testing\".\n",
    "\n",
    "I've provided the correct answers for the predictee so you can compute the accuracy of that prediction. Because the tree above is human-friendly you can do this without writing code. In the tree, the \"yes\" branch is printed first, then the \"no branch\".\n",
    "\n",
    "(But, if you want some coding practice, then implement it in Python.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=[('ieced','s'),('_scho','ʃ'),('__cal','k'),('gic__','k'),('arcos','k'),('__che','tʃ'),('orca_','k'),('duca_','k'),('__cir','s'),('__cha','tʃ'),('__car','k'),('recor','k'),('focht','k'),('__cha','tʃ'),('__cha','tʃ'),('ercei','s'),('racto','k'),('uechn','k'),('recia','tʃ'),('decli','k'),('licat','k'),('rich_','k'),('pecor','k'),('dacit','s'),('rick_','k'),('decad','k'),('__cot','k'),('__cha','tʃ'),('__cen','s'),('mac__','k'),('recei','s'),('nic__','k'),('_acqu','k'),('anca_','k'),('ouch_','tʃ'),('__car','k'),('isch_','ʃ'),('__cla','k'),('__chi','tʃ'),('_mcle','k')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answers for the test data here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
